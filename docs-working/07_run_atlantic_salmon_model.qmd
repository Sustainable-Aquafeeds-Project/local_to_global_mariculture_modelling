---
title: "Run aquaculture model for Atlantic salmon"
format: html
editor: source
---

# Introduction

This document runs the aquaculture model for Atlantic salmon farms. It processes farm location data, species parameters, and conducts sensitivity analyses to understand the impact of various parameters on fish and farm growth measures.

```{r setup, include=F}
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(sf)
library(terra)
library(furrr)
library(future)
library(tictoc)
library(fs)
library(conflicted)
library(readxl)
library(units)
library(qs)
library(here)
library(targets)
conflicted::conflicts_prefer(dplyr::filter(), dplyr::select(), .quiet = T)

here("src") %>% 
  list.files(pattern = "\\.R$", full.names = TRUE) %>% 
  str_subset("map", negate = T) %>% 
  walk(source)
```

```{r filenames}
#| code-summary: Set up global filenames

farm_script <- here("targets/_targets_farmruns.R")
farm_store <- here("targets/_targets_farmruns")
sens_script <- here("targets/_targets_sensitivities.R")
sens_store <- here("targets/_targets_sensitivities")

# Filenames
species_params_excel <- c(file = file.path(input_species_param_path, "Species.xlsx"), sheet = "Atlantic salmon")
pop_params_excel <- c(file = file.path(input_species_param_path, "Population.xlsx"))
farm_locations_parquet <- file.path(input_farm_coords_path, "farm_coords.parquet")
farm_coords_file <- file.path(output_farm_data_path, "farm_coords.qs")
farm_geometry_file <- file.path(output_farm_data_path, "farm_geometry.qs")
farm_ts_data_file <- file.path(output_farm_data_path, "farm_ts_data.qs")
species_params_file <- file.path(output_species_data_path, "species_params.qs")
sens_params_file <- file.path(output_species_data_path, "sens_params.qs")
pop_params_file <- file.path(output_species_data_path, "pop_params.qs")
feed_params_file <- file.path(output_species_data_path, "feed_params.qs")
farm_harvest_file <- file.path(output_farm_data_path, "farm_harvest_size.qs")
```

# Data for targets pipelines

Much of the actual analysis is run through targets pipelines. Therefore, we need to make sure that the files going into those pipelines are correct and up to date.

```{r farm-coordinates, eval=F}
#| code-summary: Load and process farm coordinate data with appropriate timing parameters for Northern and Southern hemisphere farms

times_N <- c("t_start" = 121, "t_end" = 121+547, "dt" = 1)
times_S <- c("t_start" = 274, "t_end" = 274+547, "dt" = 1)

farm_coords <- farm_locations_parquet %>% 
  read_parquet() %>% 
  mutate(t_start = case_when(lat > 0 ~ times_N['t_start'], TRUE ~ times_S['t_start']), 
          t_end = case_when(lat > 0 ~ times_N['t_end'], TRUE ~ times_S['t_end']),
          t_start = unname(t_start),
          t_end = unname(t_end))

qsave(farm_coords, farm_coords_file)

# Also save geometry for later
file.path(input_farm_coords_path, "atlantic_salmon_locations_w_temps.qs") %>% 
  qread() %>% 
  dplyr::filter(day == "day_1") %>% 
  dplyr::select(farm_id, geometry, country) %>% 
  qsave(farm_geometry_file)
```

## Farm Time Series Data

Process Sea Surface Temperature (SST) data for each farm location.

```{r farm-sst-data, eval=F}
#| code-summary: Process Sea Surface Temperature (SST) data for each farm location.

farms_to_omit <- qread(sprintf(file.path(input_farm_coords_path, "%s_farms_to_omit.qs"), this_species))
farm_SST_data <- read_parquet(file.path(input_farm_sst_path, "farm_SST_extracted.parquet"))
farm_IDs <- farm_SST_data %>%
  filter(!farm_id %in% farms_to_omit) %>%
  distinct(farm_id) %>%
  pull(farm_id)

farm_ts_data <- farm_SST_data %>%
  rename(farm_ID = farm_id) %>% 
  select(c(farm_ID, day, temp_c)) %>%
  mutate(day = str_split_i(day, "day_", 2) %>% as.integer())

qsave(farm_ts_data, farm_ts_data_file)
```

## Species and population parameters

```{r species-parameters, eval=F}
#| code-summary: Load species-specific parameters from Excel file

species_params <- readxl::read_excel(
  path = species_params_excel["file"], 
  sheet = species_params_excel["sheet"]
)
vals <- species_params$Value
names(vals) <- species_params$Quantity
species_params <- vals[!is.na(vals)]
qsave(species_params, species_params_file)
```

```{r population-parameters, eval=F}
#| code-summary: Load population-specific parameters

pop_params <- readxl::read_excel(path = pop_params_excel["file"])
vals <- pop_params$Value
names(vals) <- pop_params$Quantity
pop_params <- vals[!is.na(vals)]
qsave(pop_params, pop_params_file)
```

## Feed digestibility

```{r variable-digestibility}
#| code-summary: Show the variability in feed digestibility generated by sampling digestibility values.
#| fig-cap: Whole-feed digestibility of the feeds run in this analysis. 

# tar_make(names = c("feed_params", "feed_names"), script = farm_script, store = farm_store)

tar_load(feed_params, store = farm_store)
tar_load(feed_names, store = farm_store)

feed_params <- feed_params %>% 
  map2_dfr(., feed_names, function(fp, fn) {
    ld <- filter(fp[["Lipids"]], macro != 0)
    ld <- sum(ld$proportion * ld$digest)/sum(ld$proportion)

    cd <- filter(fp[["Carbohydrates"]], macro != 0)
    cd <- sum(cd$proportion * cd$digest)/sum(cd$proportion)

    pd <- filter(fp[["Proteins"]], macro != 0)
    pd <- sum(pd$proportion * pd$digest)/sum(pd$proportion)

    data.frame(
      protein_digestibility = pd,
      carb_digestibility = cd,
      lipid_digestibility = ld,
      feed_name = fn
    )
  }) %>% 
  pivot_longer(
    cols = contains("digestibility"),
    names_to = "macro", 
    values_to = "digestibility"
  ) %>% 
  mutate(
    macro = factor(
      macro, 
      levels = c("protein_digestibility", "lipid_digestibility", "carb_digestibility"),
      labels = c("protein", "lipid", "carb")
    ),
    feed_name = as.factor(feed_name)
  )

feed_params %>% 
  ggplot(aes(x = macro, y = digestibility, fill = feed_name)) +
  geom_col(position = position_dodge()) +
  theme_classic()
```

# Farm Harvest Size Calculations

First, let's test the pipeline to make sure that the feed input data is being processed correctly.

```{r check-farmrun-pipeline}
#| code-summary: Check that the pipeline will run correctly

# Sys.setenv(TAR_PROJECT = "project_farmruns")

tar_validate(script = farm_script, store = farm_store)
# tar_prune(script = farm_script, store = farm_store)

tar_make(test_reference_feed, script = farm_script, store = farm_store)
tar_load_globals(script = farm_script)
reference_feed_name
tar_read(test_reference_feed, store = farm_store) # if weight ~ 0 then feeds aren't processing properly
```

Calculations of expected harvest sizes for each farm (using default feed and no Monte-Carlo variation) are done in the targets pipeline. This sets a "target population" for each farm and grounds the growth somewhat.

```{r farm-harvest-size, eval=F}
#| code-summary: Run limited pipeline for harvest sizes and testing

# Sys.setenv(TAR_PROJECT = "project_farmruns")

# Get harvest sizes for all farms
tar_make(names = contains(c("harvest_size", "feed_params")), script = farm_script, store = farm_store)
```

```{r save-harvest-size}
#| code-summary: Save harvest size data

harvest_size <- tar_read(harvest_size_chunked, store = farm_store) %>% 
  bind_rows()

ggplot(harvest_size, aes(x = weight)) +
  geom_histogram(fill = "salmon", alpha = 0.75, colour = "black")

qsave(harvest_size, farm_harvest_file)
```

# Sensitivity analysis

Run the sensitivity runs using the targets pipeline - note that this can take a long time. The species parameters (n=19) are run for a single fish in  a sample number of farms (n=272) and each factor (3). The population parameters (n=6) are run for 1000 fish in a sample number of farms (271) and each factor (3). 

```{r check-sensitivity-pipeline}
#| code-summary: Check that the sensitivities pipeline is running correctly

# Sys.setenv(TAR_PROJECT = "project_sensitivities")
# tar_validate(script = farm_script, store = farm_store)
# tar_visnetwork(targets_only = T, script = farm_script, store = farm_store)

tar_make(test_reference_feed, script = sens_script, store = sens_store)
tar_read(test_reference_feed, store = sens_store) # if weight ~ 0 then feeds aren't processing properly, fix before proceeding
```

```{r sensitivity-run}
#| code-summary: Run sensitivity pipeline

# Sys.setenv(TAR_PROJECT = "project_sensitivities")

tar_make(contains("spec"), seconds_meta_append = 300, script = sens_script, store = sens_store)
tar_make(contains("pop"), seconds_meta_append = 300, script = sens_script, store = sens_store)
# tar_prune(script = sens_script, store = sens_store)
```

## Process sensitivity results

Combine and visualise sensitivity analysis results.

```{r sensitivity-results}
#| code-summary: Plot and save sensitivity results

tar_load(sens_results_spec, store = sens_store)
tar_load(sens_results_pop, store = sens_store)

sens_results <- rbind(
  sens_results_pop,
  sens_results_spec
)

qsave(sens_results, file.path(output_sens_data_path, paste0("sens_results_all.qs")))

sens_measures <- levels(sens_results$measure)
sens_results_figfiles <- file.path(output_sens_data_path, paste0("sens_plot_", sens_measures, ".png"))

for (sm in seq_along(sens_measures)) {
  p <- sens_results %>% 
    filter(measure == sens_measures[sm]) %>% 
    ggplot(aes(x = adj_param, y = mean_sens, ymin = mean_sens-sd_sens, ymax = mean_sens+sd_sens)) +
    geom_col(fill = "salmon", alpha = 0.65, colour = "black") +
    geom_errorbar(width = 0.5) +
    coord_flip() +
    theme_classic()
  ggsave(sens_results_figfiles[sm])
}
```

# Run farms

The following code runs the main pipeline in two stages. The pre-run prepares all the pre-requisites for the main results targets, to reduce load. The main run shortcuts past all the pre-run targets and does the main model runs and results processing. 

```{r run-farms-pipeline, eval=F}
#| code-summary: Run the main pipeline

# Sys.setenv(TAR_PROJECT = "project_farmruns")

# Pre-run to reduce load
tar_make(
  names = contains(c("data", "feed")),
  script = farm_script, store = farm_store
)

# tar_outdated(script = farm_script, store = farm_store)

# Run whole pipeline
tar_make(
  seconds_meta_append = 300,
  script = farm_script, store = farm_store
)
```

Load objects from the targets pipeline and save them as .qs files, which are much quicker to load down the line. This takes a few minutes to gather all the targets.

```{r all-farms-cohorts-results}
#| code-summary: Save pipeline objects as files

Sys.setenv(TAR_PROJECT = "project_farmruns")
farm_IDs_chunked <- tar_read(farm_IDs_chunked, script = farm_script, store = farm_store) %>% unname() %>% unlist()
tar_load(feed_names, script = farm_script, store = farm_store)

tar_load(farm_full_results, script = farm_script, store = farm_store)
total_ends <- bind_rows(lapply(farm_full_results, `[[`, "total_ends"))
cohort_results <- bind_rows(lapply(farm_full_results, `[[`, "cohort_results_daily"))

qsave(total_ends, file.path(output_model_farm_path, "total_ends.qs"))
qsave(cohort_results, file.path(output_model_cohort_path, "cohort_results.qs"))
rm(farm_full_results)
```
